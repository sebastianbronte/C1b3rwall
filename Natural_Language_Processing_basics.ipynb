{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Natural Language Processing. Chapter I. Basics</h1>\n",
    "<!--<h2>Introduction, tokens, etc</h2>-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing (NLP) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the text is processed ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The text can be processed by a machine as it is with other types of data (images, sound, video, etc), as it can be represented with numbers, and thus it can be exploited so as to be processed by machines.</p>\n",
    "<p>The particularity is the way that natural text is written: It is written by humans to be read by humans as well, so the code is known by speakers of the same language, and that is not the case of a machine unless it is taught to do so</p>\n",
    "<p>To represent the charcters of text, there are some representations. Each character in a text document is in fact represented in a machine using look up tables: ASCII (the most basic ones, UTF-8 and other encodings to handle pictograms, accented characters or special characters for specific languages, such as 'ç', 'ñ', etc</p>\n",
    "<p>When dealing with text written by humans there are also additional dificulties, such as that, as humans we do some mistakes or write things in a non-standard ways. Therefore, it can be interpreted as noise or deviations of the commonly used rules of that language. This also complicates the analisys. Of course, not all the humans in the world speak the same language, so translations might be needed to represent the same meaning to make other people understand the underlying message</p>\n",
    "<p>After having seen all of this and given thah there are more neuances that complicates the understanding of the language by a machine, how do we start processing the text automatically?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main trends in which written text can be divided to better perform the analysis:\n",
    "* character analysis\n",
    "* word analysis\n",
    "\n",
    "Each of those have their pros and cons.\n",
    "Regarding per-character analysis the set of possible basic symbols to memorize is short, but the possible combinations are far more intractable. On the other hand, this way does not restrict the posibility of producing new words in case the application needs it.\n",
    "With respect to the per word analysis, the set of possible symbols to analyze is as big as th amount of words we want to consider. The combinations between symbols are also big, but not as potentially big as a per-character combinations. With this, the meaning analysis is closer as a word, or the main part of it, contans a basic meaning which has more restrictions, so it is easier to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing libraries: nltk / spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade setuptools\n",
    "# NLTK library\n",
    "!python3 -m pip install -U nltk\n",
    "# Spacy library \n",
    "!python3 -m pip install -U spacy\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the imports:\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In order to process the words and just the words, we need a way to separate them into minimum units that can mean something.</p>\n",
    "<p>As a first approach, taking the text and splitting the contents with spaces, sometimes taking punctuation characters as ',', '.' or others to separate or sometimes not, depending on the application.</p>\n",
    "<p>Sometimes, prefixes and suffixes can also be considered sometimes as tokens, as they add additional meaning to the word they are connected to but they can be de-coupled.</p>\n",
    "<p>Tokens are not limited to words but sentences can also be divided</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self-driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n",
      "“\n",
      "I\n",
      "can\n",
      "tell\n",
      "you\n",
      "very\n",
      "senior\n",
      "CEOs\n",
      "of\n",
      "major\n",
      "American\n",
      "car\n",
      "companies\n",
      "would\n",
      "shake\n",
      "my\n",
      "hand\n",
      "and\n",
      "turn\n",
      "away\n",
      "because\n",
      "I\n",
      "wasn\n",
      "’\n",
      "t\n",
      "worth\n",
      "talking\n",
      "to\n",
      ",\n",
      "”\n",
      "said\n",
      "Thrun\n",
      ",\n",
      "in\n",
      "an\n",
      "interview\n",
      "with\n",
      "Recode\n",
      "earlier\n",
      "this\n",
      "week\n",
      ".\n",
      "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\n",
      "\n",
      "\n",
      "“I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use nltk / spacy to tokenize a sentence and see how the segmentation\n",
    "# is performed\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "# for nltk\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "for token in tokens :\n",
    "    print(token)\n",
    "    \n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print (sentence)\n",
    "    print('\\n')\n",
    "\n",
    "# just for you to know, there are special tokenizers in nltk,\n",
    "# i.e. TweetTokenizer, which takes into account specific things\n",
    "# to work for the tweet type of text: more informal, short, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self\n",
      "-\n",
      "driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n",
      "“\n",
      "I\n",
      "can\n",
      "tell\n",
      "you\n",
      "very\n",
      "senior\n",
      "CEOs\n",
      "of\n",
      "major\n",
      "American\n",
      "car\n",
      "companies\n",
      "would\n",
      "shake\n",
      "my\n",
      "hand\n",
      "and\n",
      "turn\n",
      "away\n",
      "because\n",
      "I\n",
      "was\n",
      "n’t\n",
      "worth\n",
      "talking\n",
      "to\n",
      ",\n",
      "”\n",
      "said\n",
      "Thrun\n",
      ",\n",
      "in\n",
      "an\n",
      "interview\n",
      "with\n",
      "Recode\n",
      "earlier\n",
      "this\n",
      "week\n",
      ".\n",
      "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\n",
      "\n",
      "\n",
      "“I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, in an interview with Recode earlier this week.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning, stop words removal, etc ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There are some words that do not add any meaning, but only serve to complete structures. There words are some of the following: punctuation, pronouns, possesives, demonstratives, reflexives, some verbs (to be), articles, particles, etc.</p>\n",
    "<p>It can be seen that those words can have more meaning that it can be seen but in fact, for a high level analysis, for instance in classification, sentiment analysis, etc, the words that contain the most of the meaning are not those, but words like nouns, adjetives, etc.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use nltk, and show the head of the stop words. Apply to a sentence\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(nltk_stopwords)\n",
    "\n",
    "filtered_nltk = [w for w in tokens if w not in nltk_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spacy to do the same\n",
    "filtered_spacy = [w for w in doc if not w.is_stop]\n",
    "print(filtered_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization / Stemming ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this operations we remove the declinations / flexions of the word giving only the most meaningful part of it\n",
    "\n",
    "With the lematization we have a dictionary in which we map the possible combinations and the lema to give as output\n",
    "\n",
    "The stemmization cuts the word according to some rules and it can yield sometimes to incorrect / imprecise word reductions\n",
    "\n",
    "For instance, if we remove the number and gender of some words we get the actual meaning\n",
    "\n",
    "|   word  | lemmatization | stemming |\n",
    "|---------|---------------|----------|\n",
    "| niñas   | niño          | niñ      |\n",
    "| niñez   | niñez         | niñ      |\n",
    "| studies | study         | studi    |\n",
    "| study   | study         | study    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nltk to do some examples of both things: lemmatization, stemming \n",
    "  \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "words = ['children', 'studies', 'study']\n",
    "for w in words:\n",
    "    print(w + ' -> ' + lemmatizer.lemmatize(w) + ' / ' + stemmer.stem(w))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for w in filtered_nltk:\n",
    "    print(w + ' -> ' + lemmatizer.lemmatize(w) + ' / ' + stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for spacy\n",
    "for w in filtered_spacy:\n",
    "    # in spacy there is no implementation of stemmers. Lemmatizers are presumed to be better than stemmers\n",
    "    print(w.text + ' -> ' + w.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regexp search ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search can be done using raw python functions, including re package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working', 'self-driving', 'talking']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "l = [w for w in tokens if re.search(\".*ing\", w)]\n",
    "print(repr(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, nltk and spacy also includes some of the basic functionalities for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workin; self-drivin; talkin\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# nltk code (for token only)\n",
    "nltkText = nltk.Text(tokens)\n",
    "print(repr(nltkText.findall(r'.*ing')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy code (for token only)\n",
    "\n",
    "# Match different spellings of token texts\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \".*ing\"}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>etc...</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS (Part Of Speech) is the acronim for the syntactic analysis of a sentence. Here the result is whether a word is a noun, a verb, an adjetive, etc.\n",
    "\n",
    "Depending on the library, the tags are not the same between libraries.\n",
    "For instance NNP is a noun in nltk and NOUN in spacy.\n",
    "It is also not treated the same way in both libraries: In nltk, POS is calculated when needed, weather in spacy, it is calculated at the beginning and left on a field in each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nltk the code is the following\n",
    "pos_nltk = nltk.pos_tag(tokens)\n",
    "for w, p in pos_nltk:\n",
    "    print(w + ' -> ' + p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for spacy the code is simpler\n",
    "for w in doc:\n",
    "    print (w.text + ' -> ' + w.pos_ + ' ' + w.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
